{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import pathlib, sys, os, random, time\n",
    "import cv2\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "from torchvision import datasets,transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm.notebook import tqdm\n",
    "import albumentations as A\n",
    "import functools\n",
    "from torchvision import models\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    " transforms.ToTensor(), # 将图片转换为Tensor,归一化至[0,1]\n",
    " # transforms.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5]) # 标准化至[-1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHES = 120\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 256\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' \n",
    "\n",
    "trfm = A.Compose([\n",
    "    A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrackData(data.Dataset):\n",
    "    \"\"\"Dataset for Crack detection\"\"\"\n",
    "    def __init__(self, data_images, data_GT,transform):\n",
    "        imgs = os.listdir(data_images)\n",
    "        self.imgs=[os.path.join(data_images,k) for k in imgs]\n",
    "        GT = os.listdir(data_GT)\n",
    "        self.GTs=[os.path.join(data_GT,k) for k in GT]\n",
    "        self.transforms=transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.imgs[index]\n",
    "        GT_path = self.GTs[index]\n",
    "        pil_img = Image.open(img_path)\n",
    "        pil_GT = Image.open(GT_path)\n",
    "        if self.transforms:\n",
    "            data = self.transforms(pil_img)\n",
    "            label = self.transforms(pil_GT)\n",
    "        else:\n",
    "            pil_img = np.asarray(pil_img)\n",
    "            pil_GT = np.asarray(pil_GT)\n",
    "            data = torch.from_numpy(pil_img)\n",
    "            label = torch.from_numpy(pil_GT)\n",
    "        return data, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CrackData('./datas/train_img','./datas/train_lab',transform)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "655\n"
     ]
    }
   ],
   "source": [
    "valid_idx, train_idx = [], []\n",
    "for i in range(len(dataset)):\n",
    "    if i % 7 == 0:\n",
    "        valid_idx.append(i)\n",
    "    else:\n",
    "        train_idx.append(i)\n",
    "   # elif i % 7 == 1:\n",
    "print(len(valid_idx))\n",
    "print(len(train_idx))\n",
    "       \n",
    "        \n",
    "train_ds = data.Subset(dataset, train_idx)\n",
    "valid_ds = data.Subset(dataset, valid_idx)\n",
    "\n",
    "# define training and validation data loaders\n",
    "loader = data.DataLoader(\n",
    "    train_ds, batch_size=4, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "vloader = data.DataLoader(\n",
    "    valid_ds, batch_size=4, shuffle=False,drop_last=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x2conv(in_channels, out_channels, inner_channels=None):\n",
    "    inner_channels = out_channels // 2 if inner_channels is None else inner_channels\n",
    "    down_conv = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, inner_channels, kernel_size=3, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(inner_channels),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(inner_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True))\n",
    "    return down_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(encoder, self).__init__()\n",
    "        self.down_conv = x2conv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, ceil_mode=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down_conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(decoder, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.up_conv = x2conv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_copy, x, interpolate=True):\n",
    "        x = self.up(x)\n",
    "\n",
    "        if (x.size(2) != x_copy.size(2)) or (x.size(3) != x_copy.size(3)):\n",
    "            if interpolate:\n",
    "                # Iterpolating instead of padding\n",
    "                x = F.interpolate(x, size=(x_copy.size(2), x_copy.size(3)),\n",
    "                                mode=\"bilinear\", align_corners=True)\n",
    "            else:\n",
    "                # Padding in case the incomping volumes are of different sizes\n",
    "                diffY = x_copy.size()[2] - x.size()[2]\n",
    "                diffX = x_copy.size()[3] - x.size()[3]\n",
    "                x = F.pad(x, (diffX // 2, diffX - diffX // 2,\n",
    "                                diffY // 2, diffY - diffY // 2))\n",
    "\n",
    "        # Concatenate\n",
    "        x = torch.cat([x_copy, x], dim=1)\n",
    "        x = self.up_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RF(nn.Module):\n",
    "    # Revised from: Receptive Field Block Net for Accurate and Fast Object Detection, 2018, ECCV\n",
    "    # GitHub: https://github.com/ruinmessi/RFBNet\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(RF, self).__init__()\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "            BasicConv2d(in_channel, out_channel, 1),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            BasicConv2d(in_channel, out_channel, 1),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(3, 1), padding=(1, 0)),\n",
    "            BasicConv2d(out_channel, out_channel, 3, padding=3, dilation=3)\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            BasicConv2d(in_channel, out_channel, 1),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(1, 5), padding=(0, 2)),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(5, 1), padding=(2, 0)),\n",
    "            BasicConv2d(out_channel, out_channel, 3, padding=5, dilation=5)\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            BasicConv2d(in_channel, out_channel, 1),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(1, 7), padding=(0, 3)),\n",
    "            BasicConv2d(out_channel, out_channel, kernel_size=(7, 1), padding=(3, 0)),\n",
    "            BasicConv2d(out_channel, out_channel, 3, padding=7, dilation=7)\n",
    "        )\n",
    "\n",
    "        self.conv_cat = BasicConv2d(4*out_channel, out_channel, 3, padding=1)\n",
    "        self.conv_res = BasicConv2d(in_channel, out_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "\n",
    "        x_cat = self.conv_cat(torch.cat((x0, x1, x2, x3), dim=1))\n",
    "\n",
    "        x = self.relu(x_cat + self.conv_res(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1):\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes,\n",
    "                              kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aggregation(nn.Module):\n",
    "    # dense aggregation, it can be replaced by other aggregation model, such as DSS, amulet, and so on.\n",
    "    # used after MSF\n",
    "    def __init__(self, channel):\n",
    "        super(aggregation, self).__init__()\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv_upsample1 = BasicConv2d(channel, channel, 3, padding=1)\n",
    "        self.conv_upsample2 = BasicConv2d(channel, channel, 3, padding=1)\n",
    "        self.conv_upsample3 = BasicConv2d(channel, channel, 3, padding=1)\n",
    "        self.conv_upsample4 = BasicConv2d(channel, channel, 3, padding=1)\n",
    "        self.conv_upsample5 = BasicConv2d(2*channel, 2*channel, 3, padding=1)\n",
    "\n",
    "        self.conv_concat2 = BasicConv2d(2*channel, 2*channel, 3, padding=1)\n",
    "        self.conv_concat3 = BasicConv2d(3*channel, 3*channel, 3, padding=1)\n",
    "        self.conv4 = BasicConv2d(3*channel, 3*channel, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(3*channel, 1, 1)\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        x1_1 = x1\n",
    "        x2_1 = self.conv_upsample1(self.upsample(x1)) * x2\n",
    "        x3_1 = self.conv_upsample2(self.upsample(self.upsample(x1))) \\\n",
    "               * self.conv_upsample3(self.upsample(x2)) * x3\n",
    "\n",
    "        x2_2 = torch.cat((x2_1, self.conv_upsample4(self.upsample(x1_1))), 1)\n",
    "        x2_2 = self.conv_concat2(x2_2)\n",
    "\n",
    "        x3_2 = torch.cat((x3_1, self.conv_upsample5(self.upsample(x2_2))), 1)\n",
    "        x3_2 = self.conv_concat3(x3_2)\n",
    "\n",
    "        x = self.conv4(x3_2)\n",
    "        x = self.conv5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class h_sigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = nn.ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "class h_swish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordAtt(nn.Module):\n",
    "    def __init__(self, inp, oup, groups=32):\n",
    "        super(CoordAtt, self).__init__()\n",
    "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
    "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "        mip = max(8, inp // groups)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inp, mip, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(mip)\n",
    "        self.conv2 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(mip, oup, kernel_size=1, stride=1, padding=0)\n",
    "        self.relu = h_swish()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        n,c,h,w = x.size()\n",
    "        x_h = self.pool_h(x)\n",
    "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
    "\n",
    "        y = torch.cat([x_h, x_w], dim=2)\n",
    "        y = self.conv1(y)\n",
    "        y = self.bn1(y)\n",
    "        y = self.relu(y) \n",
    "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
    "        x_w = x_w.permute(0, 1, 3, 2)\n",
    "\n",
    "        x_h = self.conv2(x_h).sigmoid()\n",
    "        x_w = self.conv3(x_w).sigmoid()\n",
    "        x_h = x_h.expand(-1, -1, h, w)\n",
    "        x_w = x_w.expand(-1, -1, h, w)\n",
    "\n",
    "        y = identity * x_w * x_h\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, in_channels=3, channel=32):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "  # ---- ResNet Backbone ----\n",
    "        self.start_conv = x2conv(in_channels, 64)\n",
    "        self.down1 = encoder(64, 128)\n",
    "        self.down2 = encoder(128, 256)\n",
    "        self.down3 = encoder(256, 512)\n",
    "        self.down4 = encoder(512, 1024)\n",
    "        self.down5 = encoder(1024, 1024)\n",
    " # ---- Receptive Field Block like module ---- \n",
    "        self.rf1= RF(512,channel)\n",
    "        self.rf2= RF(1024,channel)\n",
    "        self.rf3= RF(1024,channel)\n",
    "        \n",
    "# ---- aggregation ---- \n",
    "        self.agg1 = aggregation(channel)\n",
    "        self.CA1 = CoordAtt(32,32)\n",
    "        self.CA2 = CoordAtt(32,32)\n",
    "        self.CA3 = CoordAtt(32,32)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(channel, 1, 1)\n",
    "        \n",
    "\n",
    "#         self.up1 = decoder(1024, 512)\n",
    "#         self.up2 = decoder(512, 256)\n",
    "#         self.up3 = decoder(256, 128)\n",
    "#         self.up4 = decoder(128, 64)\n",
    "#         self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        self._initialize_weights()\n",
    "\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x1 = self.start_conv(x)\n",
    "        S1 = self.down1(x1)\n",
    "#         print('S1',S1.size())\n",
    "        S2 = self.down2(S1)\n",
    "#         print('S2',S2.size())\n",
    "        S3 = self.down3(S2)\n",
    "#         print('S3',S3.size())\n",
    "        S4 = self.down4(S3)\n",
    "#         print('S4',S4.size())\n",
    "        S5 = self.down5(S4)\n",
    "#         print('S5',S5.size())\n",
    "        # print(x4.shape)\n",
    "        S3_RF = self.rf1(S3)\n",
    "#         print('S3_RF',S3_RF.size())\n",
    "        S4_RF = self.rf2(S4)\n",
    "#         print('S4_RF',S4_RF.size())\n",
    "        S5_RF = self.rf3(S5)\n",
    "#         print('S5_RF',S5_RF.size())\n",
    "        \n",
    "        attention_map = self.agg1(S5_RF,S4_RF,S3_RF)\n",
    "#         print('attention_map',attention_map.size())\n",
    "        attention_map_pred = F.interpolate(attention_map, scale_factor=8, mode='bilinear')\n",
    "#         print('attention_map_pred',attention_map_pred.size())\n",
    "        \n",
    "        #Indentification\n",
    "        S3_1 = self.CA1(S3_RF)\n",
    "#         print('S3_1',S3_1.size())\n",
    "        S3_2 = self.CA2(S3_1)\n",
    "#         print('S3_2',S3_2.size())\n",
    "        S3_3 = self.CA3(S3_2)\n",
    "#         print('S3_3',S3_3.size())\n",
    "        \n",
    "        S4_1 = self.CA1(S4_RF)\n",
    "#         print('S4_1',S4_1.size())\n",
    "        S4_2 = self.CA2(S4_1)\n",
    "#         print('S4_2',S4_2.size())\n",
    "        S4_3 = self.CA3(S4_2)\n",
    "#         print('S4_3',S4_3.size())\n",
    "        \n",
    "        S5_1 = self.CA1(S5_RF)\n",
    "#         print('S5_1',S5_1.size())\n",
    "        S5_2 = self.CA2(S5_1)\n",
    "#         print('S5_2',S5_2.size())\n",
    "        S5_3 = self.CA3(S5_2)\n",
    "        S5_3 = self.conv5(S5_3)\n",
    "#         print('S5_3',S5_3.size())\n",
    "        \n",
    "        \n",
    "        guidance_g = F.interpolate(attention_map, scale_factor=0.25, mode='bilinear')\n",
    "#         print('guidance_g',guidance_g.size())\n",
    "        S5_F = S5_3 + guidance_g\n",
    "#         print('S5_F',S5_F.size()) #S5_3 torch.Size([10, 1, 7, 7])\n",
    "        S5_pred = F.interpolate(S5_F, scale_factor=32, mode='bilinear')\n",
    "#         print('S5_pred',S5_pred.size())\n",
    "        \n",
    "        S4_F = F.interpolate(S5_F, scale_factor=2, mode='bilinear')\n",
    "        S4_3 = self.conv5(S4_3)\n",
    "        S4_F = S4_3 + S4_F\n",
    "        S4_pred = F.interpolate(S4_F, scale_factor=16, mode='bilinear')\n",
    "#         print('S4_pred',S4_pred.size())\n",
    "        \n",
    "        S3_F = F.interpolate(S4_F, scale_factor=2, mode='bilinear')\n",
    "        S3_3 = self.conv5(S3_3)\n",
    "        S3_F = S3_3 + S3_F\n",
    "        S3_pred = F.interpolate(S3_F, scale_factor=8, mode='bilinear')\n",
    "#         print('S3_pred',S3_pred.size())\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "#         x = self.up1(x4, x)\n",
    "#         x = self.up2(x3, x)\n",
    "#         x = self.up3(x2, x)\n",
    "#         x = self.up4(x1, x)\n",
    "#         x = self.final_conv(x)\n",
    "\n",
    "        return attention_map_pred, S5_pred, S4_pred, S3_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation(model, loader, loss_fn):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for image, target in loader:\n",
    "        image, target = image.to(DEVICE), target.float().to(DEVICE)\n",
    "        #output = model(image)['out']\n",
    "        output = model(image)\n",
    "        loss1 = loss_fn(output[0], target)\n",
    "        loss2 = loss_fn(output[1], target)\n",
    "        loss3 = loss_fn(output[2], target)\n",
    "        loss4 = loss_fn(output[3], target)\n",
    "        loss = loss1 + loss2+loss2+loss3\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    return np.array(losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 224, 224])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = torch.rand(10, 3, 224, 224)\n",
    "result = model(images)\n",
    "result[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                 lr=1e-4, weight_decay=1e-3)\n",
    "\n",
    "class SoftDiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1., dims=(-2,-1)):\n",
    "\n",
    "        super(SoftDiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.dims = dims\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        tp = (x * y).sum(self.dims)\n",
    "        fp = (x * (1 - y)).sum(self.dims)\n",
    "        fn = ((1 - x) * y).sum(self.dims)\n",
    "        \n",
    "        dc = (2 * tp + self.smooth) / (2 * tp + fp + fn + self.smooth)\n",
    "        dc = dc.mean()\n",
    "        return 1 - dc\n",
    "    \n",
    "bce_fn = nn.BCEWithLogitsLoss()\n",
    "dice_fn = SoftDiceLoss()\n",
    "\n",
    "def loss_fn(y_pred, y_true):\n",
    "    bce = bce_fn(y_pred, y_true)\n",
    "#     dice = dice_fn(y_pred.sigmoid(), y_true)\n",
    "    return bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Train | Valid\n",
      "Epoch |  Loss |  Loss | Time, m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\AiCode\\lib\\site-packages\\ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca254873cb042238e87e35ad27466f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     1│  1.671│  1.303│  2.24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f356f1cbeda4cafa37dd8e0edee0612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=163.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     2│  1.141│  0.968│  2.08\n"
     ]
    }
   ],
   "source": [
    "header = r'''\n",
    "        Train | Valid\n",
    "Epoch |  Loss |  Loss | Time, m\n",
    "'''\n",
    "#          Epoch         metrics            time\n",
    "raw_line = '{:6d}' + '\\u2502{:7.3f}'*2 + '\\u2502{:6.2f}'\n",
    "print(header)\n",
    "\n",
    "EPOCHES = 2\n",
    "best_loss = 10\n",
    "for epoch in range(1, EPOCHES+1):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    for image, target in tqdm_notebook(loader):\n",
    "        \n",
    "        image, target = image.to(DEVICE), target.float().to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        #output = model(image)['out']\n",
    "        output = model(image)\n",
    "        loss1 = loss_fn(output[0], target)\n",
    "        loss2 = loss_fn(output[1], target)\n",
    "        loss3 = loss_fn(output[2], target)\n",
    "        loss4 = loss_fn(output[3], target)\n",
    "        loss = loss1+ loss2 + loss3 + loss4\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        # print(loss.item())\n",
    "        \n",
    "    vloss = validation(model, vloader, loss_fn)\n",
    "    print(raw_line.format(epoch, np.array(losses).mean(), vloss,\n",
    "                              (time.time()-start_time)/60**1))\n",
    "    losses = []\n",
    "    \n",
    "    if vloss < best_loss:\n",
    "        best_loss = vloss\n",
    "        torch.save(model.state_dict(), 'Tunnel_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for HED:\n\tUnexpected key(s) in state_dict: \"conv1_1_down.weight\", \"conv1_1_down.bias\", \"conv1_2_down.weight\", \"conv1_2_down.bias\", \"conv2_1_down.weight\", \"conv2_1_down.bias\", \"conv2_2_down.weight\", \"conv2_2_down.bias\", \"conv3_1_down.weight\", \"conv3_1_down.bias\", \"conv3_2_down.weight\", \"conv3_2_down.bias\", \"conv3_3_down.weight\", \"conv3_3_down.bias\", \"conv4_1_down.weight\", \"conv4_1_down.bias\", \"conv4_2_down.weight\", \"conv4_2_down.bias\", \"conv4_3_down.weight\", \"conv4_3_down.bias\", \"conv5_1_down.weight\", \"conv5_1_down.bias\", \"conv5_2_down.weight\", \"conv5_2_down.bias\", \"conv5_3_down.weight\", \"conv5_3_down.bias\". \n\tsize mismatch for score_dsn1.weight: copying a param with shape torch.Size([1, 21, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 64, 1, 1]).\n\tsize mismatch for score_dsn2.weight: copying a param with shape torch.Size([1, 21, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 128, 1, 1]).\n\tsize mismatch for score_dsn3.weight: copying a param with shape torch.Size([1, 21, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 256, 1, 1]).\n\tsize mismatch for score_dsn4.weight: copying a param with shape torch.Size([1, 21, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 512, 1, 1]).\n\tsize mismatch for score_dsn5.weight: copying a param with shape torch.Size([1, 21, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 512, 1, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-966be874b698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mDEVICE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'cuda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'cpu'\u001b[0m \u001b[1;31m#使用GPU或者cpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./RCF_Crack500_best.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#加载模型的参数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#模型送到当前设备中\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\AiCode\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    828\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 830\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    831\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HED:\n\tUnexpected key(s) in state_dict: \"conv1_1_down.weight\", \"conv1_1_down.bias\", \"conv1_2_down.weight\", \"conv1_2_down.bias\", \"conv2_1_down.weight\", \"conv2_1_down.bias\", \"conv2_2_down.weight\", \"conv2_2_down.bias\", \"conv3_1_down.weight\", \"conv3_1_down.bias\", \"conv3_2_down.weight\", \"conv3_2_down.bias\", \"conv3_3_down.weight\", \"conv3_3_down.bias\", \"conv4_1_down.weight\", \"conv4_1_down.bias\", \"conv4_2_down.weight\", \"conv4_2_down.bias\", \"conv4_3_down.weight\", \"conv4_3_down.bias\", \"conv5_1_down.weight\", \"conv5_1_down.bias\", \"conv5_2_down.weight\", \"conv5_2_down.bias\", \"conv5_3_down.weight\", \"conv5_3_down.bias\". \n\tsize mismatch for score_dsn1.weight: copying a param with shape torch.Size([1, 21, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 64, 1, 1]).\n\tsize mismatch for score_dsn2.weight: copying a param with shape torch.Size([1, 21, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 128, 1, 1]).\n\tsize mismatch for score_dsn3.weight: copying a param with shape torch.Size([1, 21, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 256, 1, 1]).\n\tsize mismatch for score_dsn4.weight: copying a param with shape torch.Size([1, 21, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 512, 1, 1]).\n\tsize mismatch for score_dsn5.weight: copying a param with shape torch.Size([1, 21, 1, 1]) from checkpoint, the shape in current model is torch.Size([1, 512, 1, 1])."
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' #使用GPU或者cpu\n",
    "model.load_state_dict(torch.load(\"./HED_best.pth\")) #加载模型的参数\n",
    "model.to(DEVICE) #模型送到当前设备中\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = os.listdir('C://Users//78731//Desktop//csnet//DeepCrackTest//test_img')\n",
    "img_name = test_imgs\n",
    "test_imgs=[os.path.join('C://Users//78731//Desktop//csnet//DeepCrackTest//test_img',k) for k in test_imgs]\n",
    "test_imgs\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_data in test_imgs:\n",
    "    img=Image.open(img_data)\n",
    "    img = transform(img)\n",
    "    img.unsqueeze_(0)\n",
    "    img = img.to(DEVICE)\n",
    "    output = model(img)\n",
    "    save_image(output, img_name[i])\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
